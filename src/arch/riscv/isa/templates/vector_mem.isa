def template VMemMacroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VMemTemplateMacroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    %(reg_idx_arr_decl)s;
public:
    %(class_name)s(ExtMachInst _machInst);
    using %(base_class)s::generateDisassembly;
};

}};

def template VleConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width);
    const uint32_t sew = getSew(_machInst.vtype8.vsew);
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    panic_if(nf * emul > 8, "invalid nf and emul");


    StaticInstPtr microop;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
        this->microops.push_back(microop);
    }
    VectorMicroInfo vmi;
    if (nf == 1) { // sequence load
        const uint32_t num_microops = ceil((float)this->vl / elem_num_per_vreg);
        for (int i = 0; i < num_microops; ++i) {
            vmi.rs = i * elem_num_per_vreg;
            vmi.re = std::min((i+1) * elem_num_per_vreg, this->vl);
            vmi.microVd = VD + i;
            vmi.fn = 0;
            vmi.offset = (i * VLEN) / 8;
            assert(vmi.re > vmi.rs);
            assert(vmi.microVd < 32);
            microop = new %(class_name)sMicro(_machInst, i, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsLoad);
            this->microops.push_back(microop);
        }
    }
    else { // segment load
        for (int i=0; i < this->vl; i++) {
            for (int fn=0; fn < nf; fn++) {
                // baseAddr = Rs1 + (i*nf + fn) * eew / 8
                // (vd + fn * emul)<eew>[i] = mem<eew>[baseAddr];
                vmi.rs = i;
                vmi.re = i+1;
                vmi.fn = fn;
                vmi.microVd = elem_gen_idx(VD + fn * emul, i, eew/8);
                vmi.offset = (i*nf+fn) * eew / 8;
                microop = new %(class_name)sMicro(_machInst, i*fn, vmi);
                microop->setDelayedCommit();
                microop->setFlag(IsLoad);
                this->microops.push_back(microop);
            }
        }
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VleMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[3];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, 0,
                     _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVd);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVd));
        if (!_machInst.vm) {
            setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0));
        }

        this->flags[IsVector] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;

};

}};

def template VleMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    Addr EA;
    const uint32_t eew = width_EEW(this->machInst.width);
    EA = Rs1 + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    const std::vector<bool> byte_enable(mem_size, true);
    Fault fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size, memAccessFlags,
                              byte_enable);
    if (fault != NoFault)
        return fault;

    COPY_OLD_VD(1);

    size_t ei;
    for (size_t i = 0; i < vmi.re - vmi.rs; i++) {
        uint32_t vdElemIdx = (vmi.rs % (VLEN / eew)) + i;
        ei = i + vmi.rs;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};

def template VleMicroInitiateAcc {{

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    %(op_src_decl)s;
    %(op_rd)s;

    Addr EA;
    const uint32_t eew = width_EEW(this->machInst.width);
    EA = Rs1 + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    const std::vector<bool> byte_enable(mem_size, true);
    Fault fault = initiateMemRead(xc, EA, mem_size, memAccessFlags,
                                  byte_enable);
    return fault;
}

}};

def template VleMicroCompleteAcc {{

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
                            Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());

    COPY_OLD_VD(1);

    const uint32_t eew = width_EEW(this->machInst.width);
    size_t ei;
    for (size_t i = 0; i < vmi.re - vmi.rs; i++) {
        uint32_t vdElemIdx = (vmi.rs % (VLEN / eew)) + i;
        ei = i + vmi.rs;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return NoFault;
}

}};

def template VleffConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    %(set_reg_idx_arr)s;
    %(constructor)s;

    const int32_t micro_vlmax = VLEN / width_EEW(_machInst.width);
    const uint32_t num_microops = ceil((float) this->vl / (micro_vlmax));
    int32_t remaining_vl = this->vl;
    int32_t micro_vl = std::min(remaining_vl, micro_vlmax);
    StaticInstPtr microop;

    if (micro_vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
        this->microops.push_back(microop);
    } else {
        for (int i = 0; i < num_microops && micro_vl > 0; ++i) {
            microop = new %(class_name)sMicro(_machInst, micro_vl, i);
            microop->setDelayedCommit();
            microop->setFlag(IsLoad);
            this->microops.push_back(microop);
            micro_vl = std::min(remaining_vl -= micro_vlmax, micro_vlmax);
        }
        microop = new VleffEndMicroInst(_machInst, num_microops);
        this->microops.push_back(microop);
    }
    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VleffMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[3];
    RegId destRegIdxArr[2];
    std::unique_ptr<size_t> fault_elem_idx;
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microVl, uint8_t _microIdx)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microVl,
                     _microIdx)
    {
        %(set_reg_idx_arr)s;
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, _machInst.vd + _microIdx));
        _numTypedDestRegs[VecRegClass]++;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, VecMemInternalReg0 + _microIdx));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, _machInst.vd + _microIdx));
        if (!_machInst.vm) {
            setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0));
        }

        fault_elem_idx = std::make_unique<size_t>(0);
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;

};

}};

def template VleffMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    Addr EA;
    %(op_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    uint32_t mem_size = width_EEW(machInst.width) / 8 * this->microVl;
    const std::vector<bool> byte_enable(mem_size, true);
    Fault fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size, memAccessFlags,
                              byte_enable);

    %(vfof_set_code)s;
    %(vfof_zero_idx_check_code)s;
    %(vfof_get_code)s;

    const size_t micro_vlmax = vtype_VLMAX(machInst.vtype8, true);
    size_t micro_elems = VLEN / width_EEW(machInst.width);

    size_t ei;
    for (size_t i = 0; i < std::min(micro_elems, *fault_elem_idx); i++) {
        ei = i + micro_vlmax * microIdx;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};


def template VleffMicroInitiateAcc {{
Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    Addr EA;

    %(op_src_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    uint32_t mem_size = width_EEW(this->machInst.width) / 8 * this->microVl;
    const std::vector<bool> byte_enable(mem_size, true);
    Fault fault = initiateMemRead(xc, EA, mem_size, memAccessFlags,
                                  byte_enable);
    
    %(vfof_set_code)s;
    %(vfof_zero_idx_check_code)s;

    return fault;
}
}};

def template VleffMicroCompleteAcc {{
Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
                            Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());

    const size_t micro_vlmax = vtype_VLMAX(machInst.vtype8, true);
    const size_t micro_elems = VLEN / width_EEW(machInst.width);
    size_t ei;
    for (size_t i = 0; i < micro_elems; i++) {
        ei = i + micro_vlmax * microIdx;
        %(memacc_code)s;
    }

    %(vfof_get_code)s;
    %(op_wb)s;
    return NoFault;
}
}};

def template VseConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width);
    const uint32_t sew = getSew(_machInst.vtype8.vsew);
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    panic_if(nf * emul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
        this->microops.push_back(microop);
    }
    VectorMicroInfo vmi;
    if (nf == 1) {
        const uint32_t num_microops = ceil((float)this->vl / elem_num_per_vreg);
        for (int i = 0; i < num_microops; ++i) {
            vmi.rs = i * elem_num_per_vreg;
            vmi.re = std::min((i+1) * elem_num_per_vreg, this->vl);
            vmi.microVs3 = VS3 + i;
            vmi.fn = 0;
            vmi.offset = (i * VLEN) / 8;
            microop = new %(class_name)sMicro(_machInst, i, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsStore);
            this->microops.push_back(microop);
        }
    }
    else {
        for (int i=0; i < this->vl; i++) {
            for (int fn=0; fn < nf; fn++) {
                // baseAddr = Rs1 + (i*nf + fn) * eew / 8
                // mem<eew>[baseAddr] = (vs3 + fn * emul)<eew>[i];
                vmi.rs = i;
                vmi.re = i+1;
                vmi.fn = fn;
                vmi.microVs3 = elem_gen_idx(VS3 + fn * emul, i, eew/8);
                vmi.offset = (i*nf+fn) * eew / 8;
                microop = new %(class_name)sMicro(_machInst, i*fn, vmi);
                microop->setDelayedCommit();
                microop->setFlag(IsStore);
                this->microops.push_back(microop);
            }
        }
    }

    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
}

}};

def template VseMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    RegId srcRegIdxArr[3];
    RegId destRegIdxArr[0];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
        : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s,
                         0, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVs3);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs3));
        if (!_machInst.vm) {
            setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0));
        }
        this->flags[IsVector] = true;
        this->flags[IsStore] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VseMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    Addr EA;

    %(op_decl)s;
    %(op_rd)s;

    const uint32_t eew = width_EEW(this->machInst.width);
    const uint32_t eewb = eew/8;
    EA = Rs1 + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    std::vector<bool> byte_enable(mem_size, false);

    size_t ei;
    for (size_t i = 0; i < vmi.re - vmi.rs; i++) {
        uint32_t vs3ElemIdx = (vmi.rs % (VLEN / eew)) + i;
        ei = i + vmi.rs;
        if (machInst.vm || elem_mask(v0, ei)) {
            %(memacc_code)s;
            auto it = byte_enable.begin() + i * eewb;
            std::fill(it, it + eewb, true);
        }
    }

    Fault fault;
    fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA, memAccessFlags,
                         nullptr, byte_enable);
    return fault;
}

}};

def template VseMicroInitiateAcc {{

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    Addr EA;

    %(op_decl)s;
    %(op_rd)s;

    const uint32_t eew = width_EEW(this->machInst.width);
    const uint32_t eewb = eew/8;
    EA = Rs1 + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    std::vector<bool> byte_enable(mem_size, false);

    size_t ei;
    for (size_t i = 0; i < vmi.re - vmi.rs; i++) {
        uint32_t vs3ElemIdx = (vmi.rs % (VLEN / eew)) + i;
        ei = i + vmi.rs;
        if (machInst.vm || elem_mask(v0, ei)) {
            %(memacc_code)s;
            auto it = byte_enable.begin() + i * eewb;
            std::fill(it, it + eewb, true);
        }
    }

    Fault fault;
    fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA, memAccessFlags,
                         nullptr, byte_enable);
    return fault;
}

}};

def template VseMicroCompleteAcc {{

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    return NoFault;
}

}};

def template VlmConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = 8;
    const uint32_t sew = getSew(_machInst.vtype8.vsew);
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = 1;
    const uint32_t elem_num_per_vreg = VLEN / eew;
    panic_if(nf * emul > 8, "invalid nf and emul");

    this->vl = (this->vl + 7) / 8;
    StaticInstPtr microop;

    VectorMicroInfo vmi;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
    }
    else if (nf == 1) {
        vmi.rs = 0;
        vmi.re = this->vl;
        vmi.microVd = VD;
        vmi.offset = 0;
        microop = new Vle8_vMicro(_machInst, 0, vmi);
        microop->setDelayedCommit();
        microop->setFlag(IsLoad);
    }
    else {
        // TODO
        panic("not implemented");
    }
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VsmConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = 8;
    const uint32_t sew = getSew(_machInst.vtype8.vsew);
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = 1;
    const uint32_t elem_num_per_vreg = VLEN / eew;
    panic_if(nf * emul > 8, "invalid nf and emul");

    this->vl = (this->vl + 7) / 8;

    StaticInstPtr microop;

    VectorMicroInfo vmi;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
    } else {
        vmi.rs = 0;
        vmi.re = this->vl;
        vmi.microVs3 = VS3;
        vmi.offset = 0;
        microop = new Vse8_vMicro(_machInst, 0, vmi);
        microop->setDelayedCommit();
        microop->setFlag(IsStore);
    }
    this->microops.push_back(microop);

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VsWholeConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
  : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    %(set_reg_idx_arr)s;
    %(constructor)s;

    size_t NFIELDS = machInst.nf + 1;
    const int32_t micro_vlmax = VLEN / width_EEW(_machInst.width);

    StaticInstPtr microop;
    for (int i = 0; i < NFIELDS; ++i) {
        microop = new %(class_name)sMicro(_machInst, micro_vlmax, i);
        microop->setDelayedCommit();
        microop->setFlag(IsStore);
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VsWholeMicroDeclare {{

class %(class_name)s: public %(base_class)s
{
private:
    RegId destRegIdxArr[0];
    RegId srcRegIdxArr[2];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microVl, uint8_t _microIdx)
        : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s,
                         _microVl, _microIdx)
    {
        %(set_reg_idx_arr)s;
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, _machInst.vs3 + _microIdx));
        this->flags[IsVector] = true;
        this->flags[IsStore] = true;
    }
    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                        Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VsWholeMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    Addr EA;
    %(op_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    for (size_t i = 0; i < VLENB; i++) {
        %(memacc_code)s;
    }

    Fault fault = writeMemAtomicLE(xc, traceData, *(vreg_t::Container*)(&Mem),
                                   EA, memAccessFlags, nullptr);
    return fault;
}

}};

def template VsWholeMicroInitiateAcc {{

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
        Trace::InstRecord* traceData) const
{
    Addr EA;
    %(op_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    for (size_t i = 0; i < VLENB; i++) {
        %(memacc_code)s;
    }

    Fault fault = writeMemTimingLE(xc, traceData, *(vreg_t::Container*)(&Mem),
                                   EA, memAccessFlags, nullptr);
    return fault;
}

}};

def template VsWholeMicroCompleteAcc {{

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    return NoFault;
}

}};

def template VlWholeConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    %(set_reg_idx_arr)s;
    %(constructor)s;

    size_t NFIELDS = machInst.nf + 1;
    const int32_t micro_vlmax = VLEN / width_EEW(_machInst.width);

    StaticInstPtr microop;
    for (int i = 0; i < NFIELDS; ++i) {
        microop = new %(class_name)sMicro(_machInst, micro_vlmax, i);
        microop->setDelayedCommit();
        microop->setFlag(IsLoad);
        this->microops.push_back(microop);
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VlWholeMicroDeclare {{

class %(class_name)s: public %(base_class)s
{
private:
    RegId destRegIdxArr[1];
    RegId srcRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microVl, uint8_t _microIdx)
        : %(base_class)s("%(mnemonic)s_micro", _machInst, %(op_class)s,
                         _microVl, _microIdx)
    {
        %(set_reg_idx_arr)s;
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, _machInst.vd + _microIdx));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        this->flags[IsVector] = true;
        this->flags[IsLoad] = true;
    }
    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                        Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VlWholeMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    Addr EA;
    %(op_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    Fault fault = readMemAtomicLE(xc, traceData, EA,
                                  *(vreg_t::Container*)(&Mem), memAccessFlags);
    if (fault != NoFault)
        return fault;

    size_t elem_per_reg = VLEN / width_EEW(machInst.width);
    for (size_t i = 0; i < elem_per_reg; i++) {
        %(memacc_code)s;
    }

    %(op_wb)s;
    return NoFault;
}

}};

def template VlWholeMicroInitiateAcc {{

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    Addr EA;
    %(op_src_decl)s;
    %(op_rd)s;
    %(ea_code)s;

    Fault fault = initiateMemRead(xc, traceData, EA, Mem, memAccessFlags);
    return fault;
}

}};

def template VlWholeMicroCompleteAcc {{

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext* xc,
        Trace::InstRecord* traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());

    size_t elem_per_reg = VLEN / width_EEW(machInst.width);
    for (size_t i = 0; i < elem_per_reg; ++i) {
        %(memacc_code)s;
    }

    %(op_wb)s;
    return NoFault;
}

}};

def template VlStrideConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width);
    const uint32_t sew = getSew(_machInst.vtype8.vsew);
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    panic_if(nf * emul > 8, "invalid nf and emul");


    StaticInstPtr microop;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
        this->microops.push_back(microop);
    }
    VectorMicroInfo vmi;
    for (int i=0; i < this->vl; i++) {
        for (int fn=0; fn < nf; fn++) {
            // baseAddr = Rs1 + i*Rs2 + fn * eew / 8
            // mem<eew>[baseAddr] = (vd + fn * emul)<eew>[i]
            vmi.rs = i;
            vmi.re = i+1;
            vmi.fn = fn;
            vmi.microVd = elem_gen_idx(VD + fn * emul, i, eew/8);
            vmi.offset = fn * eew / 8;
            microop = new %(class_name)sMicro(_machInst, i*fn, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsLoad);
            this->microops.push_back(microop);
        }
    }

    this->microops.front()->setFirstMicroop();
    this->microops.back()->setLastMicroop();
}

}};

def template VlStrideMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    // rs1, rs2, vd, vm
    RegId srcRegIdxArr[4];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVd);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs2));
        // We treat agnostic as undistrubed
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVd));
        if (!_machInst.vm) {
            setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0));
        }
        this->flags[IsLoad] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VlStrideMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    const uint32_t eew = width_EEW(this->machInst.width);
    EA = Rs1 + Rs2*vmi.rs + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    COPY_OLD_VD(2);

    uint32_t vdElemIdx = (vmi.rs % (VLEN / eew));
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        const std::vector<bool> byte_enable(mem_size, true);
        fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size,
                                memAccessFlags, byte_enable);
        if (fault != NoFault)
            return fault;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};

def template VlStrideMicroInitiateAcc {{

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    %(op_src_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    const uint32_t eew = width_EEW(this->machInst.width);
    EA = Rs1 + Rs2*vmi.rs + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    size_t ei = vmi.rs;
    bool need_load = machInst.vm || elem_mask(v0, ei);
    const std::vector<bool> byte_enable(mem_size, need_load);
    fault = initiateMemRead(xc, EA, mem_size, memAccessFlags, byte_enable);

    return fault;
}

}};

def template VlStrideMicroCompleteAcc {{

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext *xc,
                            Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    COPY_OLD_VD(2);

    const uint32_t eew = width_EEW(this->machInst.width);
    uint32_t vdElemIdx = (vmi.rs % (VLEN / eew));
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());
        %(memacc_code)s;
    }

    %(op_wb)s;
    return NoFault;
}

}};

def template VsStrideConstructor {{

%(class_name)s::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width);
    const uint32_t sew = getSew(_machInst.vtype8.vsew);
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    const uint32_t elem_num_per_vreg = VLEN / eew;
    panic_if(nf * emul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
        this->microops.push_back(microop);
    }
    VectorMicroInfo vmi;
    for (int i=0; i < this->vl; i++) {
        for (int fn=0; fn < nf; fn++) {
            // baseAddr = Rs1 + i*Rs2 + fn * eew / 8
            // mem<eew>[baseAddr] = (vs3 + fn * emul)<eew>[i];
            vmi.rs = i;
            vmi.re = i+1;
            vmi.fn = fn;
            vmi.microVs3 = elem_gen_idx(VS3 + fn * emul, i, eew/8);
            vmi.offset = fn * eew / 8;
            microop = new %(class_name)sMicro(_machInst, i*fn, vmi);
            microop->setDelayedCommit();
            microop->setFlag(IsStore);
            this->microops.push_back(microop);
        }
    }

    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
}

}};

def template VsStrideMicroDeclare {{

class %(class_name)s : public %(base_class)s
{
private:
    // rs1, rs2, vs3, vm
    RegId srcRegIdxArr[4];
    RegId destRegIdxArr[0];
public:
    %(class_name)s(ExtMachInst _machInst, uint8_t _microIdx, VectorMicroInfo& _vmi)
        : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s,  _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.microVs3);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs2));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs3));
        if (!_machInst.vm) {
            setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0));
        }
        this->flags[IsStore] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VsStrideMicroExecute {{

Fault
%(class_name)s::execute(ExecContext *xc, Trace::InstRecord *traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    const uint32_t eew = width_EEW(this->machInst.width);
    const uint32_t eewb = eew/8;
    EA = Rs1 + Rs2 * vmi.rs + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    uint32_t vs3ElemIdx = (vmi.rs % (VLEN / eew));
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        const std::vector<bool> byte_enable(mem_size, true);
        %(memacc_code)s;
        fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA,
                             memAccessFlags, nullptr, byte_enable);
    }
    return fault;
}

}};

def template VsStrideMicroInitiateAcc {{

Fault
%(class_name)s::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    %(op_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;

    const uint32_t eew = width_EEW(this->machInst.width);
    const uint32_t eewb = eew/8;
    EA = Rs1 + Rs2 * vmi.rs + vmi.offset;
    uint32_t mem_size = (vmi.re - vmi.rs) * eew / 8;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if(!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs - 1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    uint32_t vs3ElemIdx = (vmi.rs % (VLEN / eew));
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        const std::vector<bool> byte_enable(mem_size, true);
        %(memacc_code)s;
        fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA,
                            memAccessFlags, nullptr, byte_enable);
    }
    return fault;
}

}};

def template VsStrideMicroCompleteAcc {{

Fault
%(class_name)s::completeAcc(PacketPtr pkt, ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    return NoFault;
}

}};

def template VlIndexConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width); // vs2 : index[i]
    const uint32_t sew = getSew(_machInst.vtype8.vsew); // vs3 : stored elem
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    uint32_t flmul = vflmul < 1 ? 1 : vflmul;
    const uint32_t elem_num_per_vreg = VLEN / sew;
    panic_if(nf * flmul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
        this->microops.push_back(microop);
    }
    VectorMicroInfo vmi;
    for (int i = 0; i < this->vl; i++) {
        for (int fn = 0; fn < nf; fn++) {
            // baseAddr = Rs1 + fn * sew / 8
            // (vd + fn * flmul)<sew>[i] = mem<sew>[baseAddr + vs2<eew>[i]]
            vmi.fn = fn;
            vmi.rs = i;
            vmi.re = i+1;
            vmi.microVd = elem_gen_idx(VD + fn * flmul, i, sew/8);
            vmi.microVs2 = elem_gen_idx(VS2, i, eew / 8);
            vmi.offset = fn * sew / 8;
            assert(vmi.microVd < 32);
            microop = new %(class_name)sMicro<ElemType>(machInst, i * fn, vmi);
            microop->setFlag(IsDelayedCommit);
            microop->setFlag(IsLoad);
            this->microops.push_back(microop);
        }
    }

    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
    this->flags[IsVector] = true;
}

}};

def template VlIndexMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // rs1, vs2, vd, vm
    RegId srcRegIdxArr[4];
    RegId destRegIdxArr[1];
public:
    %(class_name)s(ExtMachInst _machInst, uint32_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        vmi = _vmi;
        assert(~vmi.fn);
        assert(~vmi.microVd);
        assert(~vmi.microVs2);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setDestRegIdx(_numDestRegs++, RegId(VecRegClass, vmi.microVd));
        _numTypedDestRegs[VecRegClass]++;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs2));
        // We treat agnostic as undistrubed
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVd));
        if (!_machInst.vm) {
            setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0));
        }
        this->flags[IsLoad] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VlIndexMicroExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext *xc,
    Trace::InstRecord *traceData)const
{
    using vu = std::make_unsigned_t<ElemType>;
    %(op_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / width_EEW(machInst.width));
    %(ea_code)s;


    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    COPY_OLD_VD(2);

    constexpr uint8_t elem_size = sizeof(vu);
    uint32_t mem_size = elem_size;
    const std::vector<bool> byte_enable(mem_size, true);

    uint32_t vdElemIdx = vmi.rs % (VLENB / elem_size);
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        fault = xc->readMem(EA, Mem.as<uint8_t>(), mem_size,
                                memAccessFlags, byte_enable);
        if (fault != NoFault)
            return fault;
        %(memacc_code)s;
    }

    %(op_wb)s;
    return fault;
}

}};

def template VlIndexMicroInitiateAcc {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    using vu = std::make_unsigned_t<ElemType>;
    %(op_src_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / width_EEW(machInst.width));
    %(ea_code)s;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    constexpr uint8_t elem_size = sizeof(vu);
    uint32_t mem_size = elem_size;
    size_t ei = vmi.rs;
    bool need_load = machInst.vm || elem_mask(v0, ei);
    const std::vector<bool> byte_enable(mem_size, need_load);
    fault = initiateMemRead(xc, EA, mem_size, memAccessFlags, byte_enable);
    return fault;
}

}};

def template VlIndexMicroCompleteAcc {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::completeAcc(PacketPtr pkt, ExecContext *xc,
                            Trace::InstRecord *traceData) const
{
    using vu = std::make_unsigned_t<ElemType>;
    %(op_decl)s;
    %(op_rd)s;

    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    COPY_OLD_VD(2);

    constexpr uint32_t elem_size = sizeof(vu);
    uint32_t vdElemIdx = vmi.rs % (VLENB / elem_size);
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        memcpy(Mem.as<uint8_t>(), pkt->getPtr<uint8_t>(), pkt->getSize());
        %(memacc_code)s;
    }

    %(op_wb)s;
    return NoFault;
}

}};

def template VsIndexConstructor {{

template<typename ElemType>
%(class_name)s<ElemType>::%(class_name)s(ExtMachInst _machInst)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s)
{
    const int32_t nf = _machInst.nf + 1;
    const uint32_t eew = width_EEW(_machInst.width); // vs2 : index[i]
    const uint32_t sew = getSew(_machInst.vtype8.vsew); // vs3 : stored elem
    float vflmul = getVflmul(_machInst.vtype8.vlmul);
    const uint32_t emul = get_emul(eew, sew, vflmul, false);
    uint32_t flmul = vflmul < 1 ? 1 : vflmul;
    const uint32_t elem_num_per_vreg = VLEN / sew;
    panic_if(nf * flmul > 8, "invalid nf and emul");

    StaticInstPtr microop;
    if (this->vl == 0) {
        microop = new VectorNopMicroInst(_machInst);
        this->microops.push_back(microop);
    }
    VectorMicroInfo vmi;
    for (int i=0; i<this->vl; i++) {
        // can be optimized
        for (int fn=0; fn<nf; fn++) {
            // baseAddr = Rs1 + fn * sew / 8
            // mem<sew>[baseAddr + vs2<eew>[i]] = (vs3 + fn * flmul)<sew>[i]
            vmi.fn = fn;
            vmi.rs = i;
            vmi.re = i+1;
            vmi.microVs2 = elem_gen_idx(VS2, i, eew / 8);
            vmi.microVs3 = elem_gen_idx(VS3 + fn * flmul, i, sew / 8);
            vmi.offset = fn * sew / 8;
            assert(vmi.microVs2 < 32);
            assert(vmi.microVs3 < 32);
            microop = new %(class_name)sMicro<ElemType>(machInst, i * fn, vmi);
            microop->setFlag(IsDelayedCommit);
            microop->setFlag(IsStore);
            this->microops.push_back(microop);
        }
    }
    this->microops.front()->setFlag(IsFirstMicroop);
    this->microops.back()->setFlag(IsLastMicroop);
    this->flags[IsVector] = true;
}

}};

def template VsIndexMicroDeclare {{

template<typename ElemType>
class %(class_name)s : public %(base_class)s
{
private:
    // rs1, vs2, vs3, vm
    RegId srcRegIdxArr[4];
    RegId destRegIdxArr[0];
public:
    %(class_name)s(ExtMachInst _machInst, uint32_t _microIdx, VectorMicroInfo& _vmi)
    : %(base_class)s("%(mnemonic)s", _machInst, %(op_class)s, _microIdx)
    {
        %(set_reg_idx_arr)s;
        vmi = _vmi;
        assert(~vmi.fn);
        assert(~vmi.microVs2);
        assert(~vmi.microVs3);
        assert(~vmi.offset);
        _numSrcRegs = 0;
        _numDestRegs = 0;
        setSrcRegIdx(_numSrcRegs++, RegId(IntRegClass, _machInst.rs1));
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs2));
        // We treat agnostic as undistrubed
        setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, vmi.microVs3));
        if (!_machInst.vm) {
            setSrcRegIdx(_numSrcRegs++, RegId(VecRegClass, 0));
        }
        this->flags[IsStore] = true;
    }

    Fault execute(ExecContext *, Trace::InstRecord *) const override;
    Fault initiateAcc(ExecContext *, Trace::InstRecord *) const override;
    Fault completeAcc(PacketPtr, ExecContext *,
                      Trace::InstRecord *) const override;
    using %(base_class)s::generateDisassembly;
};

}};

def template VsIndexMicroExecute {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::execute(ExecContext *xc,
    Trace::InstRecord *traceData)const
{
    using vu = std::make_unsigned_t<ElemType>;

    %(op_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / width_EEW(machInst.width));
    %(ea_code)s;
    constexpr uint8_t elem_size = sizeof(vu);
    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    uint32_t mem_size = elem_size;
    const std::vector<bool> byte_enable(mem_size, true);

    uint32_t vs3ElemIdx = vmi.rs % (VLEN / getSew(machInst.vtype8.vsew));
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        %(memacc_code)s;
        fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA,
                             memAccessFlags, nullptr, byte_enable);
    }
    return fault;
}

}};

def template VsIndexMicroInitiateAcc {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::initiateAcc(ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    using vu = std::make_unsigned_t<ElemType>;
    %(op_src_decl)s;
    %(op_rd)s;

    Fault fault = NoFault;
    Addr EA;
    uint32_t vs2ElemIdx = vmi.rs % (VLEN / width_EEW(machInst.width));
    %(ea_code)s;
    constexpr uint8_t elem_size = sizeof(vu);
    RiscvISA::vreg_t tmp_v0;
    uint8_t *v0;
    if (!machInst.vm) {
        xc->getRegOperand(this, _numSrcRegs-1, &tmp_v0);
        v0 = tmp_v0.as<uint8_t>();
    }

    constexpr uint8_t mem_size = elem_size;
    const std::vector<bool> byte_enable(mem_size, true);

    uint32_t vs3ElemIdx = vmi.rs % (VLEN / getSew(machInst.vtype8.vsew));
    size_t ei = vmi.rs;
    if (machInst.vm || elem_mask(v0, ei)) {
        %(memacc_code)s;
        fault = xc->writeMem(Mem.as<uint8_t>(), mem_size, EA,
                             memAccessFlags, nullptr, byte_enable);
    }
    return fault;
}

}};

def template VsIndexMicroCompleteAcc {{

template<typename ElemType>
Fault
%(class_name)s<ElemType>::completeAcc(PacketPtr pkt, ExecContext* xc,
                            Trace::InstRecord* traceData) const
{
    return NoFault;
}

}};

def template VMemTemplateDecodeBlock {{

switch(machInst.vtype8.vsew) {
    case 0b000: {
        return new %(class_name)s<uint8_t>(machInst);
    }
    case 0b001: {
        return new %(class_name)s<uint16_t>(machInst);
    }
    case 0b010: {
        return new %(class_name)s<uint32_t>(machInst);
    }
    case 0b011: {
        return new %(class_name)s<uint64_t>(machInst);
    }
    default: GEM5_UNREACHABLE;
}

}};

def template VleffDecodeBlock {{
    return new %(class_name)s(machInst);
}};
